# TechWatch - Guide de D√©marrage Rapide

## üöÄ D√©marrage Ultra-Rapide (Docker)

### Pr√©requis
- Docker 24+ et Docker Compose 2+
- 4GB RAM minimum
- Ports disponibles : 3000, 8000, 5432, 6379

### Lancement Complet (1 commande)

```bash
# Lancer TOUS les services (frontend, backend, DB, Redis, Celery)
docker-compose up -d

# V√©rifier que tout tourne
docker-compose ps
```

**Services d√©marr√©s :**
- ‚úÖ PostgreSQL (port 5433) - Ports modifi√©s pour √©viter conflits
- ‚úÖ Redis (port 6379)
- ‚úÖ Backend FastAPI (port 8001)
- ‚úÖ Celery Worker (scraping)
- ‚úÖ Celery Beat (scheduler 6h)
- ‚úÖ Frontend Next.js (port 3001)

### Acc√®s aux Services

| Service | URL | Description |
|---------|-----|-------------|
| **Frontend** | http://localhost:3001 | Dashboard Next.js |
| **Backend API** | http://localhost:8001 | FastAPI REST API |
| **API Docs** | http://localhost:8001/docs | Swagger UI interactive |
| **PostgreSQL** | localhost:5433 | Base de donn√©es |
| **Redis** | localhost:6379 | Cache & queue |

> **Note**: Les ports par d√©faut ont √©t√© modifi√©s (5433 au lieu de 5432, 8001 au lieu de 8000, 3001 au lieu de 3000) pour √©viter les conflits avec d'√©ventuels services locaux.

### Premiers Tests

#### 1. V√©rifier l'API Backend
```bash
# Health check
curl http://localhost:8001/health

# Lister les sources disponibles
curl http://localhost:8001/api/sources
```

#### 2. D√©clencher un Scraping Manuel
```bash
curl -X POST http://localhost:8001/api/scraping/trigger \
  -H "Content-Type: application/json" \
  -d '{"keywords": ["python", "AI", "blockchain"]}'

# R√©cup√©rer le task_id de la r√©ponse et v√©rifier le statut
curl http://localhost:8001/api/scraping/status/{task_id}
```

#### 3. Acc√©der au Frontend
Ouvrir http://localhost:3001 dans votre navigateur.

### Arr√™ter les Services

```bash
# Arr√™t propre
docker-compose down

# Arr√™t + suppression des volumes (‚ö†Ô∏è perd les donn√©es)
docker-compose down -v
```

---

## üîß D√©veloppement Local (sans Docker)

### Backend (FastAPI)

#### Installation
```bash
cd backend

# Installer Poetry si n√©cessaire
curl -sSL https://install.python-poetry.org | python3 -

# Installer les d√©pendances
poetry install

# T√©l√©charger les mod√®les spaCy
poetry run python -m spacy download en_core_web_lg
poetry run python -m spacy download fr_core_news_lg
```

#### Configuration Backend
```bash
# Utiliser le fichier .env local (SQLite)
cd backend
cp .env.example .env

# √âditer .env si besoin (API keys, etc.)
nano .env
```

#### Migrations Database
```bash
# Appliquer les migrations
poetry run alembic upgrade head

# Cr√©er une nouvelle migration (si modifications de models)
poetry run alembic revision --autogenerate -m "description"
```

#### Seed Data (Sources initiales)
```bash
poetry run python -m app.scripts.seed_sources
```

#### Lancer le Backend
```bash
# Mode dev avec hot-reload
poetry run uvicorn app.main:app --reload --port 8000

# Avec log d√©taill√©
poetry run uvicorn app.main:app --reload --port 8000 --log-level debug
```

#### Lancer Celery (dans des terminaux s√©par√©s)

**Terminal 1 - Worker:**
```bash
cd backend
poetry run celery -A app.tasks.celery_app worker --loglevel=info
```

**Terminal 2 - Beat (scheduler):**
```bash
cd backend
poetry run celery -A app.tasks.celery_app beat --loglevel=info
```

#### Tests Backend
```bash
# Tous les tests
poetry run pytest

# Avec coverage
poetry run pytest --cov=app tests/

# Test sp√©cifique
poetry run pytest tests/test_scrapers/test_reddit_plugin.py

# Verbose
poetry run pytest -v -s
```

#### Linting & Formatting
```bash
# Check linting
poetry run ruff check .

# Auto-fix
poetry run ruff check --fix .

# Formatting
poetry run ruff format .
```

---

### Frontend (Next.js)

#### Installation
```bash
cd frontend

# Installer les d√©pendances
npm install

# Ou avec pnpm/yarn
pnpm install
```

#### Configuration Frontend
```bash
# Variables d'env (d√©j√† dans .env root)
NEXT_PUBLIC_API_URL=http://localhost:8000
```

#### Lancer le Frontend
```bash
# Mode dev avec hot-reload
npm run dev

# Sur un port custom
npm run dev -- -p 3001
```

#### Build Production
```bash
# Build optimis√©
npm run build

# Lancer en mode production
npm run start
```

#### Tests Frontend
```bash
# Lancer les tests
npm run test

# Mode watch
npm run test -- --watch

# UI interactive
npm run test:ui
```

#### Linting & Formatting
```bash
# ESLint
npm run lint

# Prettier
npm run format
```

---

## üìä Database (PostgreSQL)

### Connexion Directe

```bash
# Via Docker
docker-compose exec postgres psql -U techwatch_user -d techwatch

# Local (si PostgreSQL install√©)
psql -h localhost -U techwatch_user -d techwatch
```

### Commandes SQL Utiles

```sql
-- Lister les tables
\dt

-- Voir le sch√©ma d'une table
\d+ articles

-- Compter les articles
SELECT COUNT(*) FROM articles;

-- Top 10 articles par score
SELECT title, score, category FROM articles
ORDER BY score DESC LIMIT 10;

-- Articles scrap√©s aujourd'hui
SELECT COUNT(*) FROM articles
WHERE scraped_at::date = CURRENT_DATE;

-- Voir les sources actives
SELECT name, type, last_scraped_at FROM sources
WHERE is_active = true;

-- Statistiques par cat√©gorie
SELECT category, COUNT(*), AVG(score)
FROM articles
GROUP BY category;

-- Vue Power BI : stats quotidiennes
SELECT * FROM vw_articles_daily_stats
ORDER BY date DESC LIMIT 7;

-- Quitter
\q
```

### Backup & Restore

```bash
# Backup complet
docker-compose exec postgres pg_dump -U techwatch_user techwatch > backup_$(date +%Y%m%d).sql

# Backup via Docker sans exec
docker-compose exec -T postgres pg_dump -U techwatch_user techwatch > backup.sql

# Restore
docker-compose exec -T postgres psql -U techwatch_user techwatch < backup.sql

# Backup uniquement le sch√©ma
docker-compose exec postgres pg_dump -U techwatch_user --schema-only techwatch > schema.sql

# Backup uniquement les donn√©es
docker-compose exec postgres pg_dump -U techwatch_user --data-only techwatch > data.sql
```

### R√©initialiser la DB

```bash
# ‚ö†Ô∏è ATTENTION : Supprime TOUTES les donn√©es
docker-compose down -v
docker-compose up -d postgres
# Les tables seront recr√©√©es via init.sql au d√©marrage
```

---

## üîç Logs & Debugging

### Logs Docker

```bash
# Tous les services
docker-compose logs -f

# Service sp√©cifique
docker-compose logs -f backend
docker-compose logs -f celery_worker
docker-compose logs -f frontend

# 100 derni√®res lignes
docker-compose logs --tail=100 backend

# Logs depuis une date
docker-compose logs --since 2024-01-01T10:00:00 backend
```

### Logs Backend (local)

```bash
# Logs dans la console (uvicorn)
poetry run uvicorn app.main:app --reload --log-level debug

# Logs Celery
poetry run celery -A app.tasks.celery_app worker --loglevel=debug
```

### Debugging

```bash
# Entrer dans un container
docker-compose exec backend /bin/bash
docker-compose exec postgres /bin/bash

# Inspecter les processus
docker-compose exec backend ps aux

# V√©rifier les variables d'env
docker-compose exec backend env | grep DATABASE_URL
```

---

## üõ†Ô∏è Commandes Utiles

### Docker

```bash
# Rebuild apr√®s changement de d√©pendances
docker-compose up -d --build

# Rebuild un service sp√©cifique
docker-compose up -d --build backend

# Voir l'usage des ressources
docker stats

# Nettoyer les images inutilis√©es
docker system prune -a

# Voir les volumes
docker volume ls

# Supprimer les volumes orphelins
docker volume prune
```

### Celery

```bash
# Voir les tasks actives
poetry run celery -A app.tasks.celery_app inspect active

# Voir les tasks registered
poetry run celery -A app.tasks.celery_app inspect registered

# Voir les workers connect√©s
poetry run celery -A app.tasks.celery_app inspect stats

# Purger toutes les tasks en queue
poetry run celery -A app.tasks.celery_app purge

# Flower (monitoring UI, si install√©)
poetry run celery -A app.tasks.celery_app flower
```

### Redis

```bash
# Connexion Redis
docker-compose exec redis redis-cli

# Commandes Redis
PING
KEYS *
GET key_name
FLUSHALL  # ‚ö†Ô∏è Vide tout Redis
```

---

## üîê Configuration API Keys

### Anthropic (Claude) - Pour les r√©sum√©s IA

1. Cr√©er un compte : https://console.anthropic.com
2. G√©n√©rer une API key
3. Ajouter dans `.env` :
```bash
ANTHROPIC_API_KEY=sk-ant-api03-votre-cl√©-ici
```

### Reddit - Pour scraper r/programming, etc.

1. Cr√©er une app : https://www.reddit.com/prefs/apps
2. Choisir "script" type
3. Noter `client_id` et `client_secret`
4. Ajouter dans `.env` :
```bash
REDDIT_CLIENT_ID=votre_client_id
REDDIT_CLIENT_SECRET=votre_client_secret
REDDIT_USER_AGENT=TechWatch/1.0
```

### SMTP (Email Digest) - Optionnel

Pour Gmail avec App Password :
```bash
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=votre-email@gmail.com
SMTP_PASSWORD=votre-app-password
EMAIL_FROM=noreply@techwatch.local
```

---

## üéØ Workflows de D√©veloppement

### Ajouter une Nouvelle Source de Scraping

1. Cr√©er le plugin :
```bash
cd backend/app/scrapers/plugins
touch newsource.py
```

2. Impl√©menter le scraper (voir `SCRAPING_SYSTEM.md`)

3. Tester :
```bash
poetry run pytest tests/test_scrapers/test_newsource.py
```

4. Ajouter la source en DB :
```sql
INSERT INTO sources (name, type, base_url, config) VALUES
('NewSource', 'newsource', 'https://example.com', '{"max_articles": 50}');
```

### Ajouter un Endpoint API

1. Cr√©er le schema :
```bash
cd backend/app/schemas
nano new_feature.py
```

2. Cr√©er la route :
```bash
cd backend/app/api
nano new_feature.py
```

3. Tester :
```bash
curl -X POST http://localhost:8000/api/new_feature -H "Content-Type: application/json" -d '{}'
```

### Modifier la DB

1. Modifier le model :
```bash
nano backend/app/models/article.py
```

2. G√©n√©rer la migration :
```bash
cd backend
poetry run alembic revision --autogenerate -m "add column xyz"
```

3. Appliquer :
```bash
poetry run alembic upgrade head
```

---

## ‚ö° M√©triques de Performance

### Objectifs

- **API Response** : < 200ms (P95)
- **Scraping** : 100+ articles/jour
- **NLP Scoring** : < 2s par article
- **Frontend Load** : < 2s (First Contentful Paint)

### Monitoring

```bash
# Backend response time
curl -w "@curl-format.txt" -o /dev/null -s http://localhost:8000/api/articles

# Scraping stats (derni√®res 24h)
docker-compose exec postgres psql -U techwatch_user -d techwatch -c \
  "SELECT COUNT(*) FROM articles WHERE scraped_at > NOW() - INTERVAL '24 hours';"

# Worker performance
docker stats techwatch_celery_worker
```

---

## üêõ Troubleshooting

### Backend ne d√©marre pas

```bash
# V√©rifier les logs
docker-compose logs backend

# Probl√®me de DB ? V√©rifier PostgreSQL
docker-compose exec postgres pg_isready

# Probl√®me de migrations ?
docker-compose exec backend poetry run alembic current
docker-compose exec backend poetry run alembic upgrade head
```

### Celery ne scrape pas

```bash
# Worker actif ?
docker-compose exec celery_worker celery -A app.tasks.celery_app inspect active

# Redis OK ?
docker-compose exec redis redis-cli PING

# V√©rifier les sources actives
docker-compose exec postgres psql -U techwatch_user -d techwatch -c \
  "SELECT name, is_active FROM sources;"
```

### Frontend erreur 500

```bash
# Backend accessible ?
curl http://localhost:8000/health

# CORS configur√© ?
# V√©rifier dans backend/app/main.py : allow_origins=["http://localhost:3000"]
```

### Aucun article scrap√©

```bash
# V√©rifier les keywords actifs
docker-compose exec postgres psql -U techwatch_user -d techwatch -c \
  "SELECT * FROM keywords WHERE is_active = true;"

# D√©clencher manuellement
curl -X POST http://localhost:8000/api/scraping/trigger \
  -H "Content-Type: application/json" \
  -d '{"keywords": ["python"]}'

# Voir les erreurs dans scraping_runs
docker-compose exec postgres psql -U techwatch_user -d techwatch -c \
  "SELECT * FROM scraping_runs ORDER BY started_at DESC LIMIT 5;"
```

---

## üìö Documentation Compl√®te

- **Architecture** : `.claude/CLAUDE.md`
- **Syst√®me de Scraping** : `backend/SCRAPING_SYSTEM.md`
- **API Reference** : http://localhost:8000/docs
- **README Principal** : `README.md`

---

## üö¶ Statut des Services (Checklist)

Apr√®s `docker-compose up -d`, v√©rifier :

- [ ] PostgreSQL : `docker-compose exec postgres pg_isready`
- [ ] Redis : `docker-compose exec redis redis-cli PING`
- [ ] Backend : `curl http://localhost:8000/health`
- [ ] Frontend : Ouvrir http://localhost:3000
- [ ] Celery Worker : `docker-compose logs celery_worker | grep "ready"`
- [ ] Celery Beat : `docker-compose logs celery_beat | grep "beat"`

**Tout est OK ?** ‚úÖ Vous √™tes pr√™t √† d√©velopper !

---

**Derni√®re mise √† jour** : 2024-12-31
**Version** : 1.0.0
